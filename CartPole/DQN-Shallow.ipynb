{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN PyTorch\n",
    "<img src=\"https://miro.medium.com/max/1206/1*nb61CxDTTAWR1EJnbCl1cA.png\" \n",
    "     alt=\"Pseudo code\" \n",
    "     style=\"width:800px;height:550px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our environment is deterministic, so all equations presented here are also formulated deterministically for the sake of simplicity. In RL literature, they would also contain expectations over stochastic transitions in the environment. \n",
    "\n",
    "Loss function: Huber loss. Huber loss acts like the mean squared error when the error is small, but like the MAE when the error is large. This makes it more robust to ouliers when the estimates of Q are very noisy. We calculate this over a batch of transitions, B, sampled from the replay memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"DQN Network\n",
    "        Args:\n",
    "            input_dim (int): `state` dimension.\n",
    "                `state` is 2-D tensor of shape (n, input_dim)\n",
    "            output_dim (int): Number of actions.\n",
    "                Q_value is 2-D tensor of shape (n, output_dim)\n",
    "            hidden_dim (int): Hidden dimension in fc layer\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.final = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns a Q_value\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 2-D tensor of shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\",\n",
    "                        field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Replay memory class\n",
    "        Args:\n",
    "            capacity (int): Max size of this memory\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.cursor = 0\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self,\n",
    "             state: np.ndarray,\n",
    "             action: int,\n",
    "             reward: int,\n",
    "             next_state: np.ndarray,\n",
    "             done: bool) -> None:\n",
    "        \"\"\"Creates `Transition` and insert\n",
    "        Args:\n",
    "            state (np.ndarray): 1-D tensor of shape (input_dim,)\n",
    "            action (int): action index (0 <= action < output_dim)\n",
    "            reward (int): reward value\n",
    "            next_state (np.ndarray): 1-D tensor of shape (input_dim,)\n",
    "            done (bool): whether this state was last step\n",
    "        \"\"\"\n",
    "        if len(self) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.cursor] = Transition(state,\n",
    "                                              action, reward, next_state, done)\n",
    "        self.cursor = (self.cursor + 1) % self.capacity\n",
    "\n",
    "    def pop(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"Returns a minibatch of `Transition` randomly\n",
    "        Args:\n",
    "            batch_size (int): Size of mini-bach\n",
    "        Returns:\n",
    "            List[Transition]: Minibatch of `Transition`\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"Agent class that choose action and train\n",
    "        Args:\n",
    "            input_dim (int): input dimension\n",
    "            output_dim (int): output dimension\n",
    "            hidden_dim (int): hidden dimension\n",
    "        \"\"\"\n",
    "        self.dqn = DQN(input_dim, output_dim, hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.dqn.parameters())\n",
    "\n",
    "    def _to_variable(self, x: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"torch.Variable syntax helper\n",
    "        Args:\n",
    "            x (np.ndarray): 2-D tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: torch variable\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "    def get_action(self, states: np.ndarray, eps: float) -> int:\n",
    "        \"\"\"Returns an action\n",
    "        Args:\n",
    "            states (np.ndarray): 2-D tensor of shape (n, input_dim)\n",
    "            eps (float): ùú∫-greedy for exploration\n",
    "        Returns:\n",
    "            int: action index\n",
    "        \"\"\"\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.choice(self.output_dim)\n",
    "        else:\n",
    "            self.dqn.train(mode=False)\n",
    "            scores = self.get_Q(states)\n",
    "            _, argmax = torch.max(scores.data, 1)\n",
    "            return int(argmax.numpy())\n",
    "\n",
    "    def get_Q(self, states: np.ndarray) -> torch.FloatTensor:\n",
    "        \"\"\"Returns `Q-value`\n",
    "        Args:\n",
    "            states (np.ndarray): 2-D Tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.FloatTensor: 2-D Tensor of shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        states = self._to_variable(states.reshape(-1, self.input_dim))\n",
    "        self.dqn.train(mode=False)\n",
    "        return self.dqn(states)\n",
    "\n",
    "    def train(self, Q_pred: torch.FloatTensor, Q_true: torch.FloatTensor) -> float:\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        Args:\n",
    "            Q_pred (torch.FloatTensor): Predicted value by the network,\n",
    "                2-D Tensor of shape(n, output_dim)\n",
    "            Q_true (torch.FloatTensor): Target value obtained from the game,\n",
    "                2-D Tensor of shape(n, output_dim)\n",
    "        Returns:\n",
    "            float: loss value\n",
    "        \"\"\"\n",
    "        self.dqn.train(mode=True)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss_fn(Q_pred, Q_true)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_helper(agent: Agent, minibatch: List[Transition], gamma: float) -> float:\n",
    "    \"\"\"Prepare minibatch and train them\n",
    "    Args:\n",
    "        agent (Agent): Agent has `train(Q_pred, Q_true)` method\n",
    "        minibatch (List[Transition]): Minibatch of `Transition`\n",
    "        gamma (float): Discount rate of Q_target\n",
    "    Returns:\n",
    "        float: Loss value\n",
    "    \"\"\"\n",
    "    states = np.vstack([x.state for x in minibatch])\n",
    "    actions = np.array([x.action for x in minibatch])\n",
    "    rewards = np.array([x.reward for x in minibatch])\n",
    "    next_states = np.vstack([x.next_state for x in minibatch])\n",
    "    done = np.array([x.done for x in minibatch])\n",
    "\n",
    "    Q_predict = agent.get_Q(states)\n",
    "    Q_target = Q_predict.clone().data.numpy()\n",
    "    Q_target[np.arange(len(Q_target)), actions] = rewards + gamma * np.max(agent.get_Q(next_states).data.numpy(), axis=1) * ~done\n",
    "    Q_target = agent._to_variable(Q_target)\n",
    "\n",
    "    return agent.train(Q_predict, Q_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env: gym.Env,\n",
    "                 agent: Agent,\n",
    "                 replay_memory: ReplayMemory,\n",
    "                 eps: float,\n",
    "                 batch_size: int) -> int:\n",
    "    \"\"\"Play an epsiode and train\n",
    "    Args:\n",
    "        env (gym.Env): gym environment (CartPole-v0)\n",
    "        agent (Agent): agent will train and get action\n",
    "        replay_memory (ReplayMemory): trajectory is saved here\n",
    "        eps (float): ùú∫-greedy for exploration\n",
    "        batch_size (int): batch size\n",
    "    Returns:\n",
    "        int: reward earned in this episode\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a = agent.get_action(s, eps)\n",
    "        s2, r, done, info = env.step(a)\n",
    "\n",
    "        total_reward += r\n",
    "\n",
    "        if done:\n",
    "            r = -1\n",
    "        replay_memory.push(s, a, r, s2, done)\n",
    "\n",
    "        if len(replay_memory) > batch_size:\n",
    "\n",
    "            minibatch = replay_memory.pop(batch_size)\n",
    "            train_helper(agent, minibatch, GAMMA)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_dim(env: gym.Env) -> Tuple[int, int]:\n",
    "    \"\"\"Returns input_dim & output_dim\n",
    "    Args:\n",
    "        env (gym.Env): gym Environment (CartPole-v0)\n",
    "    Returns:\n",
    "        int: input_dim\n",
    "        int: output_dim\n",
    "    \"\"\"\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    return input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_annealing( episode: int, max_episode: int, min_eps: float) -> float:\n",
    "    ''' Returns Œµ-greedy\n",
    "        1.0---|\\\n",
    "              | \\\n",
    "              |  \\\n",
    "        min_e +---+------->\n",
    "                  |\n",
    "                  max_episode\n",
    "    Args:\n",
    "        episode (int): Current episode (0<= episode)\n",
    "        max_episode (int): After max episode, Œµ will be 'min_eps'\n",
    "        min_eps (float): Œµ will never go below this value\n",
    "    \n",
    "    Returns:\n",
    "        float: Œµ-value\n",
    "    '''\n",
    "    slope = (min_eps - 1.0)/ max_episode\n",
    "    return max(slope * episode + 1.0, min_eps)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best code ever written:\n",
    "https://gym.openai.com/evaluations/eval_onwKGm96QkO9tJwdX7L0Gw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode:     1] Reward:  11.0 ùú∫-greedy:  1.00\n",
      "[Episode:     2] Reward:  23.0 ùú∫-greedy:  0.98\n",
      "[Episode:     3] Reward:  13.0 ùú∫-greedy:  0.96\n",
      "[Episode:     4] Reward:  40.0 ùú∫-greedy:  0.94\n",
      "[Episode:     5] Reward:  16.0 ùú∫-greedy:  0.92\n",
      "[Episode:     6] Reward:  27.0 ùú∫-greedy:  0.90\n",
      "[Episode:     7] Reward:  16.0 ùú∫-greedy:  0.88\n",
      "[Episode:     8] Reward:  12.0 ùú∫-greedy:  0.86\n",
      "[Episode:     9] Reward:  16.0 ùú∫-greedy:  0.84\n",
      "[Episode:    10] Reward:  22.0 ùú∫-greedy:  0.82\n",
      "[Episode:    11] Reward:  15.0 ùú∫-greedy:  0.80\n",
      "[Episode:    12] Reward:  20.0 ùú∫-greedy:  0.78\n",
      "[Episode:    13] Reward:  14.0 ùú∫-greedy:  0.76\n",
      "[Episode:    14] Reward:  12.0 ùú∫-greedy:  0.74\n",
      "[Episode:    15] Reward:  25.0 ùú∫-greedy:  0.72\n",
      "[Episode:    16] Reward:  11.0 ùú∫-greedy:  0.70\n",
      "[Episode:    17] Reward:  13.0 ùú∫-greedy:  0.68\n",
      "[Episode:    18] Reward:  11.0 ùú∫-greedy:  0.66\n",
      "[Episode:    19] Reward:  15.0 ùú∫-greedy:  0.64\n",
      "[Episode:    20] Reward:  12.0 ùú∫-greedy:  0.62\n",
      "[Episode:    21] Reward:  17.0 ùú∫-greedy:  0.60\n",
      "[Episode:    22] Reward:  11.0 ùú∫-greedy:  0.58\n",
      "[Episode:    23] Reward:  18.0 ùú∫-greedy:  0.56\n",
      "[Episode:    24] Reward:   8.0 ùú∫-greedy:  0.54\n",
      "[Episode:    25] Reward:  20.0 ùú∫-greedy:  0.52\n",
      "[Episode:    26] Reward:  23.0 ùú∫-greedy:  0.51\n",
      "[Episode:    27] Reward:  11.0 ùú∫-greedy:  0.49\n",
      "[Episode:    28] Reward:  10.0 ùú∫-greedy:  0.47\n",
      "[Episode:    29] Reward:  10.0 ùú∫-greedy:  0.45\n",
      "[Episode:    30] Reward:  10.0 ùú∫-greedy:  0.43\n",
      "[Episode:    31] Reward:  10.0 ùú∫-greedy:  0.41\n",
      "[Episode:    32] Reward:  15.0 ùú∫-greedy:  0.39\n",
      "[Episode:    33] Reward:   8.0 ùú∫-greedy:  0.37\n",
      "[Episode:    34] Reward:  10.0 ùú∫-greedy:  0.35\n",
      "[Episode:    35] Reward:  11.0 ùú∫-greedy:  0.33\n",
      "[Episode:    36] Reward:  13.0 ùú∫-greedy:  0.31\n",
      "[Episode:    37] Reward:  10.0 ùú∫-greedy:  0.29\n",
      "[Episode:    38] Reward:   9.0 ùú∫-greedy:  0.27\n",
      "[Episode:    39] Reward:  13.0 ùú∫-greedy:  0.25\n",
      "[Episode:    40] Reward:  11.0 ùú∫-greedy:  0.23\n",
      "[Episode:    41] Reward:   9.0 ùú∫-greedy:  0.21\n",
      "[Episode:    42] Reward:   9.0 ùú∫-greedy:  0.19\n",
      "[Episode:    43] Reward:   9.0 ùú∫-greedy:  0.17\n",
      "[Episode:    44] Reward:   8.0 ùú∫-greedy:  0.15\n",
      "[Episode:    45] Reward:   9.0 ùú∫-greedy:  0.13\n",
      "[Episode:    46] Reward:  13.0 ùú∫-greedy:  0.11\n",
      "[Episode:    47] Reward:  10.0 ùú∫-greedy:  0.09\n",
      "[Episode:    48] Reward:  10.0 ùú∫-greedy:  0.07\n",
      "[Episode:    49] Reward:  10.0 ùú∫-greedy:  0.05\n",
      "[Episode:    50] Reward:   8.0 ùú∫-greedy:  0.03\n",
      "[Episode:    51] Reward:   9.0 ùú∫-greedy:  0.01\n",
      "[Episode:    52] Reward:   9.0 ùú∫-greedy:  0.01\n",
      "[Episode:    53] Reward:   8.0 ùú∫-greedy:  0.01\n",
      "[Episode:    54] Reward:  10.0 ùú∫-greedy:  0.01\n",
      "[Episode:    55] Reward:   9.0 ùú∫-greedy:  0.01\n",
      "[Episode:    56] Reward:   8.0 ùú∫-greedy:  0.01\n",
      "[Episode:    57] Reward:   9.0 ùú∫-greedy:  0.01\n",
      "[Episode:    58] Reward:  12.0 ùú∫-greedy:  0.01\n",
      "[Episode:    59] Reward:  23.0 ùú∫-greedy:  0.01\n",
      "[Episode:    60] Reward:  31.0 ùú∫-greedy:  0.01\n",
      "[Episode:    61] Reward:  35.0 ùú∫-greedy:  0.01\n",
      "[Episode:    62] Reward:  25.0 ùú∫-greedy:  0.01\n",
      "[Episode:    63] Reward:  30.0 ùú∫-greedy:  0.01\n",
      "[Episode:    64] Reward:  37.0 ùú∫-greedy:  0.01\n",
      "[Episode:    65] Reward:  37.0 ùú∫-greedy:  0.01\n",
      "[Episode:    66] Reward:  41.0 ùú∫-greedy:  0.01\n",
      "[Episode:    67] Reward:  72.0 ùú∫-greedy:  0.01\n",
      "[Episode:    68] Reward:  63.0 ùú∫-greedy:  0.01\n",
      "[Episode:    69] Reward:  20.0 ùú∫-greedy:  0.01\n",
      "[Episode:    70] Reward:  25.0 ùú∫-greedy:  0.01\n",
      "[Episode:    71] Reward:  18.0 ùú∫-greedy:  0.01\n",
      "[Episode:    72] Reward:  19.0 ùú∫-greedy:  0.01\n",
      "[Episode:    73] Reward:  23.0 ùú∫-greedy:  0.01\n",
      "[Episode:    74] Reward:  43.0 ùú∫-greedy:  0.01\n",
      "[Episode:    75] Reward:  17.0 ùú∫-greedy:  0.01\n",
      "[Episode:    76] Reward:  28.0 ùú∫-greedy:  0.01\n",
      "[Episode:    77] Reward:  46.0 ùú∫-greedy:  0.01\n",
      "[Episode:    78] Reward:  23.0 ùú∫-greedy:  0.01\n",
      "[Episode:    79] Reward:  36.0 ùú∫-greedy:  0.01\n",
      "[Episode:    80] Reward:  26.0 ùú∫-greedy:  0.01\n",
      "[Episode:    81] Reward:  22.0 ùú∫-greedy:  0.01\n",
      "[Episode:    82] Reward:  53.0 ùú∫-greedy:  0.01\n",
      "[Episode:    83] Reward:  31.0 ùú∫-greedy:  0.01\n",
      "[Episode:    84] Reward:  41.0 ùú∫-greedy:  0.01\n",
      "[Episode:    85] Reward:  61.0 ùú∫-greedy:  0.01\n",
      "[Episode:    86] Reward:  40.0 ùú∫-greedy:  0.01\n",
      "[Episode:    87] Reward:  54.0 ùú∫-greedy:  0.01\n",
      "[Episode:    88] Reward:  63.0 ùú∫-greedy:  0.01\n",
      "[Episode:    89] Reward:  55.0 ùú∫-greedy:  0.01\n",
      "[Episode:    90] Reward:  24.0 ùú∫-greedy:  0.01\n",
      "[Episode:    91] Reward:  43.0 ùú∫-greedy:  0.01\n",
      "[Episode:    92] Reward:  71.0 ùú∫-greedy:  0.01\n",
      "[Episode:    93] Reward:  22.0 ùú∫-greedy:  0.01\n",
      "[Episode:    94] Reward:  45.0 ùú∫-greedy:  0.01\n",
      "[Episode:    95] Reward:  85.0 ùú∫-greedy:  0.01\n",
      "[Episode:    96] Reward: 140.0 ùú∫-greedy:  0.01\n",
      "[Episode:    97] Reward:  62.0 ùú∫-greedy:  0.01\n",
      "[Episode:    98] Reward:  35.0 ùú∫-greedy:  0.01\n",
      "[Episode:    99] Reward:  85.0 ùú∫-greedy:  0.01\n",
      "[Episode:   100] Reward:  60.0 ùú∫-greedy:  0.01\n",
      "[Episode:   101] Reward:  72.0 ùú∫-greedy:  0.01\n",
      "[Episode:   102] Reward:  43.0 ùú∫-greedy:  0.01\n",
      "[Episode:   103] Reward:  28.0 ùú∫-greedy:  0.01\n",
      "[Episode:   104] Reward:  55.0 ùú∫-greedy:  0.01\n",
      "[Episode:   105] Reward:  73.0 ùú∫-greedy:  0.01\n",
      "[Episode:   106] Reward:  93.0 ùú∫-greedy:  0.01\n",
      "[Episode:   107] Reward:  98.0 ùú∫-greedy:  0.01\n",
      "[Episode:   108] Reward:  65.0 ùú∫-greedy:  0.01\n",
      "[Episode:   109] Reward:  64.0 ùú∫-greedy:  0.01\n",
      "[Episode:   110] Reward:  67.0 ùú∫-greedy:  0.01\n",
      "[Episode:   111] Reward:  77.0 ùú∫-greedy:  0.01\n",
      "[Episode:   112] Reward: 100.0 ùú∫-greedy:  0.01\n",
      "[Episode:   113] Reward: 199.0 ùú∫-greedy:  0.01\n",
      "[Episode:   114] Reward:  74.0 ùú∫-greedy:  0.01\n",
      "[Episode:   115] Reward:  69.0 ùú∫-greedy:  0.01\n",
      "[Episode:   116] Reward:  45.0 ùú∫-greedy:  0.01\n",
      "[Episode:   117] Reward: 170.0 ùú∫-greedy:  0.01\n",
      "[Episode:   118] Reward:  66.0 ùú∫-greedy:  0.01\n",
      "[Episode:   119] Reward: 104.0 ùú∫-greedy:  0.01\n",
      "[Episode:   120] Reward: 126.0 ùú∫-greedy:  0.01\n",
      "[Episode:   121] Reward:  71.0 ùú∫-greedy:  0.01\n",
      "[Episode:   122] Reward: 159.0 ùú∫-greedy:  0.01\n",
      "[Episode:   123] Reward:  93.0 ùú∫-greedy:  0.01\n",
      "[Episode:   124] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   125] Reward: 192.0 ùú∫-greedy:  0.01\n",
      "[Episode:   126] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   127] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   128] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   129] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   130] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   131] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   132] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   133] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   134] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   135] Reward: 185.0 ùú∫-greedy:  0.01\n",
      "[Episode:   136] Reward: 165.0 ùú∫-greedy:  0.01\n",
      "[Episode:   137] Reward: 196.0 ùú∫-greedy:  0.01\n",
      "[Episode:   138] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   139] Reward: 185.0 ùú∫-greedy:  0.01\n",
      "[Episode:   140] Reward: 170.0 ùú∫-greedy:  0.01\n",
      "[Episode:   141] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   142] Reward: 187.0 ùú∫-greedy:  0.01\n",
      "[Episode:   143] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   144] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   145] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   146] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   147] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   148] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   149] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   150] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   151] Reward: 200.0 ùú∫-greedy:  0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-18e7e515aa7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_annealing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_EPISODES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Episode: {:5}] Reward: {:5} ùú∫-greedy: {:5.2f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-2825739c7d75>\u001b[0m in \u001b[0;36mplay_episode\u001b[1;34m(env, agent, replay_memory, eps, batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mminibatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mtrain_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-b9a991d6f23e>\u001b[0m in \u001b[0;36mtrain_helper\u001b[1;34m(agent, minibatch, gamma)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLoss\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Ronald\\Anaconda3\\envs\\game_default\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \"\"\"\n\u001b[0;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main\n",
    "GAME = 'CartPole-v0'\n",
    "HIDDEN_DIM = 12\n",
    "CAPACITY = 50000\n",
    "NUM_EPISODES = 1000\n",
    "MAX_EPISODES = 50\n",
    "MIN_EPSILON = 0.01\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "\n",
    "try:\n",
    "    env = gym.make(GAME)\n",
    "    env = gym.wrappers.Monitor(env, directory='dqn_monitors', force=True)   # Write information about agent's performance in a file with optional video recording of agent in action \n",
    "    rewards = deque(maxlen=100)\n",
    "    input_dim, output_dim = get_env_dim(env)\n",
    "    agent = Agent(input_dim, output_dim, HIDDEN_DIM)\n",
    "    replay_memory = ReplayMemory(CAPACITY)\n",
    "    \n",
    "    for i in range(NUM_EPISODES):\n",
    "        eps = epsilon_annealing(i, MAX_EPISODES, MIN_EPSILON)\n",
    "        r = play_episode(env, agent, replay_memory, eps, BATCH_SIZE)\n",
    "        print(\"[Episode: {:5}] Reward: {:5} ùú∫-greedy: {:5.2f}\".format(i + 1, r, eps))\n",
    "        \n",
    "        rewards.append(r)\n",
    "\n",
    "        if len(rewards) == rewards.maxlen:\n",
    "            if np.mean(rewards) >= 200:\n",
    "                print(\"Game cleared in {} games with {}\".format(i + 1, np.mean(rewards)))\n",
    "                break\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
