{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Policy First Visit Monte Carlo Control\n",
    "following Sutton barton book page 92\n",
    "\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/033M8.png\" \n",
    "     alt=\"Pseudo code\" \n",
    "     style=\"width:700px;height:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from collections import defaultdict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-visit MC prediction\n",
    "def OnPolicyFirstVisitMCControl(eps=0.5):\n",
    "    '''\n",
    "    Written based on Sutton and Barto algorithm\n",
    "    '''\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    buckets=(1, 1, 6, 12)\n",
    "    nA = 2\n",
    "    max_episodes = 20000\n",
    "    gamma = 1    # don't want rewards to get smaller and smaller. We don't want to limit the time\n",
    "    \n",
    "    def discretize(obs):\n",
    "        upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "    \n",
    "    def generateEpisode(policy, max_iterations):\n",
    "        '''Generates an episode\n",
    "        Ex. [(State0, Action0, Reward1), (State1, Action1, Reward2)]\n",
    "        '''\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for t in range(max_iterations):\n",
    "            state = discretize(state)\n",
    "            action = np.random.choice(2, 1, policy[state].tolist())[0]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        return episode\n",
    "            \n",
    "    state_values = np.zeros(buckets)\n",
    "    # arbitrary policy initialized with shape of the states. Only two actions. Need to sum the probability matrix to 1 tho\n",
    "    policy_shape = buckets + (nA,)\n",
    "#     policy = np.random.rand(*policy_shape)\n",
    "    policy = np.full(policy_shape, 1/nA)\n",
    "    # Q_values are just the average of the returns after many episodes\n",
    "    Q_values = np.zeros(buckets + (nA,))\n",
    "    Returns = defaultdict(list)\n",
    "    \n",
    "    for i in range(max_episodes):\n",
    "        episode = generateEpisode(policy, 200)\n",
    "        states_in_episode = list(set([sar for sar in episode]))\n",
    "        # Expected sum of returns\n",
    "        G = 0\n",
    "        for state, action, reward in states_in_episode[::-1]:\n",
    "            G = gamma * G + reward\n",
    "            Returns[(state, action)].append(G)\n",
    "            Q_values[state][action] = np.average(Returns[(state, action)])\n",
    "            optimal_action = np.argmax(Q_values[state])\n",
    "\n",
    "            # epsilon-soft policy\n",
    "            for a in range(nA):\n",
    "                if a == optimal_action:\n",
    "                    policy[state][a] = 1 - eps + eps/nA\n",
    "                else:\n",
    "                    policy[state][a] = eps/nA\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = True):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    buckets=(1, 1, 6, 12)\n",
    "    def discretize(obs):\n",
    "        upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "    \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    max_iterations = 200\n",
    "    \n",
    "    for step_idx in range(max_iterations):\n",
    "        if render and step_idx % 10 == 0:\n",
    "            env.render()\n",
    "        state = discretize(state)\n",
    "        a = np.random.choice(2, 1, policy[state].tolist())[0]\n",
    "        state, reward, done , _ = env.step(int(a))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, True) for _ in range(n)]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = OnPolicyFirstVisitMCControl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(gym.make(\"CartPole-v1\"), optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off Policy First Visit Monte Carlo Control\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/Xi0vX.png\" \n",
    "     alt=\"Pseudo code\" \n",
    "     style=\"width:700px;height:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OffPolicyMonteCarloControl(env, num_episodes, behavior_policy, gamma=1.0):\n",
    "    '''\n",
    "    Monte Carlo Control Off-Policy Control using Weight Importance Sampling\n",
    "    Finds an optimal target greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample\n",
    "        behavior_policy: The behavior to follow while generating episodes.\n",
    "            This is usually more exploratory\n",
    "        discount_factor: Gamma discount factor\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (Q, policy)\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "            policy is a function that takes an observation as an argument and returns\n",
    "            action probabilities. This is the optimal greedy policy\n",
    "    '''\n",
    "    def discretize(obs):\n",
    "        '''\n",
    "            Discretizes the continuous state values to a discrete value\n",
    "        '''\n",
    "        buckets=(1, 1, 6, 12)    \n",
    "        upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "    \n",
    "    def generateEpisode(policy, max_iterations):\n",
    "        '''Generates an episode\n",
    "        Ex. [(State0, Action0, Reward1), (State1, Action1, Reward2)]\n",
    "        '''\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for t in range(max_iterations):\n",
    "            state = discretize(state)\n",
    "            probs = behavior_policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        return episode\n",
    "    \n",
    "    def create_greedy_policy(Q):\n",
    "        '''\n",
    "        Creates a greedy policy based on Q values.\n",
    "\n",
    "        Args:\n",
    "            Q: A dictionary that maps from state -> action values\n",
    "\n",
    "        Returns:\n",
    "            A function that takes an observation as input and returns a vector\n",
    "            of the highest possible action for each observation/state\n",
    "        '''\n",
    "        def policy_fn(state):\n",
    "            A = np.zeros_like(Q[state], dtype=float)\n",
    "            best_action = np.argmax(Q[state])   # Note: breaks tie with first one\n",
    "            A[best_action] = 1.0\n",
    "            return A\n",
    "        return policy_fn\n",
    "\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # the cumulative denominator of the weighted importance sampling formula\n",
    "    # (across all episodes)\n",
    "    C = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    target_policy = create_greedy_policy(Q)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        episode = generateEpisode(target_policy, 200)\n",
    "        # Sum of discounted returns\n",
    "        G = 0\n",
    "        # Importance sampling ratio (the weights of the returns)\n",
    "        W = 1.0\n",
    "        \n",
    "        for state, action, reward in episode[::-1]: # because the episodes are reversed, the returns in the end are weighted higher\n",
    "            G = gamma * G + reward\n",
    "            C[state][action] += W\n",
    "            \n",
    "            # This improves our target policy since it's a reference to Q\n",
    "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
    "            \n",
    "            if action != np.argmax(target_policy(state)):\n",
    "                break\n",
    "            W = W * 1./behavior_policy(state)[action]   \n",
    "            # Naturally this feels weird since the behavior policy is preset and you aren't updating the behavior policy\n",
    "            # Also, here, if it were ordinary importance sampling, you'd be dividing by the number of time steps in which state s has visited\n",
    "    return Q, target_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(nA):\n",
    "    \"\"\"\n",
    "    Creates a random policy function.\n",
    "    \n",
    "    Args:\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes an observation as input and returns a vector\n",
    "        of action probabilities\n",
    "    \"\"\"\n",
    "    A = np.ones(nA, dtype=float) / nA\n",
    "    def policy_fn(observation):\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50000/50000."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "random_policy = create_random_policy(env.action_space.n)\n",
    "Q, policy = OffPolicyMonteCarloControl(env, num_episodes=50000,behavior_policy=random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, render = True):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    buckets=(1, 1, 6, 12)\n",
    "    def discretize(obs):\n",
    "        upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "    \n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    max_iterations = 200\n",
    "    \n",
    "    for step_idx in range(max_iterations):\n",
    "        if step_idx % 10 == 0:\n",
    "            print(step_idx)\n",
    "        if render:\n",
    "            env.render()\n",
    "        state = discretize(state)\n",
    "        probs = policy(state)\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, True) for _ in range(n)]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2e44ea32ee73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluate_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CartPole-v1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimal_policy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_policy' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_policy(gym.make(\"CartPole-v1\"), optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
